\documentclass{report}
\usepackage[utf8]{inputenc}

\begin{titlepage}
\title{機械学習を用いたパワーリフティングスコアの回帰予測}
\author{1M170055-1　古瀬慶大 }
\date{\today}
\end{titlepage}
\pagebreak

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\usepackage{natbib}
\usepackage[dvipdfmx]{graphicx}
\usepackage{here}


\begin{document}

\maketitle

\newpage

\section{Abstract}
Hello world


\newpage
\tableofcontents


\newpage
\section{はじめに}
パワーリフティングとは、ベーベルを持ち上げその重さを競う競技であり、ベンチプレス・スクワット・デッドリフトの３種目から構成される。無論、パワーリフティングにおいては最後の試技が終わるその瞬間まで試合の結果は分からないものであるが、各選手のデータを使用すればある程度の予測は可能になるだろう。本研究では、人工知能分野におけるアルゴリズムの一種であるニューラルネットワークを用い、パワーリフティングの記録の回帰分析の精度を向上させることを目標とする。なお、今回回帰予測の対象とする競技は、ベンチプレスとする。スクワットとデッドリフトは共に大腿四頭筋の関与が大きく、第3章でもこの2つの種目は重量の相関がとても大きい事が分かっている。それ故、それら2種目と比べて比較的筋肉群の関与が少ないベンチプレスの回帰予測精度向上を本研究の目標とする。

一般的に、ニューラルネットワークは微分可能な変換を繋げて作られる計算グラフのことであり、この名称は人間の脳の構造を模したものである。人間の脳はニューロンのネットワークとなっており、ニューロンは電気信号によって他のニューロンに情報を伝達する。そして、複数の信号を入力として受け取り、1つの値を出力する学習モデルをパーセプトロンと呼ぶ。パーセプトロンにおいて、各入力値の重要度を調整する値を重み\begin{math}W\end{math}と言う時、入力値と重みの総和がある一定の値を超えると発火する指標となるものを閾値(バイアス\begin{math}b\end{math})と言う。情報の伝達において、入力層と出力層のみのモデルでは線形分離しかできないが、活性化関数と呼ばれる隠れ層を増やし様々な論理ゲートを組み合わせることで、非線形分離ができるようになる。そして、こうした深い隠れ層を持つネットワークモデルを使って学習する手法をディープラーニングと呼ぶ。

本論文の構成として、まず第3章で今回使用するデータセットに前処理を施し、第4章で、ニューラルネットワークの精度を測る指標を紹介し、第5章で学習精度を向上させるための手法をいくつか紹介した後に、第6章で実際の実験結果を述べている。そして最後に、第7章では今後の課題と謝辞で結んでいる。

\newpage
\section{データセットの前処理}


\subsection{データの概要}

今回研究に用いるデータセットはOpenPowerliftingのホームページから拝借したものであり、各競技参加者に対し以下のような情報を含んでいる。

\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{data_head.png}
\caption{データ全体}
\end{center}
\end{figure}

現状ではパワーリフティングの記録の回帰において必要のない指標も多く含まれているため、今回学習に使う指標は性別・体重・スクワットの重量・デットリフトの重量・年齢・ギア・ベンチプレスの重量のみとする。次に、データを扱いやすくするために年齢や性別などのカテゴリカル変数を適当なスカラー値に変換しする。また、今回の学習では欠損値が含まれるデータは初めから省いており、その場合におけるデータの総数は681124.である。この段階でのデータセットの中身はFigure2に示す。
\\

\begin{python}[caption=必要なデータの選別]
csv = './openpowerlifting-2020-09-06.csv'
df = pandas.read_csv(csv,index_col=0,low_memory=False)
df['Sex'] = df['Sex'].map({'M': 1, 'F': 0, 'Mx': -1})
df['Equipment'] = df['Equipment'].map({'Single-ply':0, 
'Raw':1, 'Wraps':2, 'Unlimited':3, 'Multi-ply':4})
data = df[['Sex','BodyweightKg', 'Best3SquatKg', 
'Best3DeadliftKg','Age','Equipment', 'Best3BenchKg']].
dropna(how='any')
\end{python}


\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{data_trimed.png}
\caption{使用するデータセット}
\end{center}
\end{figure}

それぞれの指標について簡単な説明を加えておく。一般的にパワーリフティングでは1回の試合において各競技3回まで試技を行う事が許されている。そのため、通常は試行回数を重ねるごとに記録は伸びる傾向にあるが、例えば2回目の試技では重量を持ち上げることに成功したが3回目の試技では失敗してしまった場合、3回目の試技欄には空白が入ることとなる。そのため、今回学習に用いるデータにおいてはそれぞれの試技において最高重量のみを抽出したものを使用している。また、パワーリフティングでは大会によって体にギア（図のEquipmentがそれにあたる）を着用する事が許されている場合があり、ギアを装着した場合は挙上重量が増加する傾向にある。

\subsection{外れ値の除去}

次に、データセットから各統計情報を可視化し、常識的に考えてありえないデータを含む行や、外れ値を含む行の削除を行う。
\\

\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{data_stats.png}
\caption{データの統計情報}
\end{center}
\end{figure}

パワーリフティングはバーベルを挙げる重量を競う競技であり、持ち上げた重量は必ず0kg以上でなければならないにも関わらず、Figure3よりそれぞれの種目にもいて各重量の最小値が0kg以下のデータが存在する事が伺える。よって、それぞれの種目において最小値が0kg以下の値が含まれる行を削除する。また、今回のデータセットには13歳未満の競技者は含まれていない筈であるにも関わらず、Figure3を見ると年齢が0歳のデータも含まれている事が分かるので、それらのデータも削除する。


\begin{python}[caption=0kg以下の重量、13歳未満の年齢を含む行の削除]
data = data.drop(data[data['Best3SquatKg'] <= 0].index)
data = data.drop(data[data['Best3DeadliftKg'] <= 0].index)
data = data.drop(data[data['Best3BenchKg'] <= 0].index)
data = data.drop(data[data['Age'] < 13].index)
\end{python}

この段階での統計情報はFigure5のようになっており、データの総数は666284.である。


\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{data_stats2.png}
\caption{0kg以下の重量、13歳未満の年齢を省いたデータの統計情報}
\end{center}
\end{figure}

後ほど詳しく説明するが、今回の学習では損失関数に二乗和誤差(Mean Square Error)を使い、この関数は外れ値の影響を強く受けやすいため、上記のデータから外れ値を取り除く必要がある。一般に、外れ値は次のように表される。

\begin{center}
\begin{math}
Lower Outlier = Q1 - (1.5 * IQR)
\end{math}
\end{center}
\begin{center}
\begin{math}
Higher Outlier = Q3 + (1.5 * IQR)
\end{math}
\end{center}


Listing3より、271g以上-39kg以下のベンチプレスの値、400kg以上0kg以下のデッドリフトの値、386kg以上-24g以下のスクワットの値を外れ値とみなされる。先ほど、0kg以下の記録を含む行は削除したため、外れ値が0以下となっているものについては考慮の対象外とする。つまり、今回削除する行は全てHigher Outlierとなる。
\\


\begin{python}[caption=外れ値の削除]
q1 = data['Best3BenchKg'].describe()['25%']
q3 = data['Best3BenchKg'].describe()['75%']
iqr = q3 - q1
outlier_min = q1 - (iqr) * 1.5
outlier_max = q3 + (iqr) * 1.5
print(round(outlier_min), round(outlier_max))
# -39 271
q1 = data['Best3DeadliftKg'].describe()['25%']
q3 = data['Best3DeadliftKg'].describe()['75%']
iqr = q3 - q1
outlier_min = q1 - (iqr) * 1.5
outlier_max = q3 + (iqr) * 1.5
print(round(outlier_min), round(outlier_max))
# 0 400
q1 = data['Best3SquatKg'].describe()['25%']
q3 = data['Best3SquatKg'].describe()['75%']
iqr = q3 - q1
outlier_min = q1 - (iqr) * 1.5
outlier_max = q3 + (iqr) * 1.5
print(round(outlier_min), round(outlier_max))
# -24 386
data = data.drop(data[data['Best3BenchKg'] >= 271].index)
data = data.drop(data[data['Best3DeadliftKg'] <= 1].index)
data = data.drop(data[data['Best3DeadliftKg'] >= 399].index)
data = data.drop(data[data['Best3SquatKg'] >= 383].index)
\end{python}


この段階でのデータの総数は645974.となり、統計情報はFigure5のようになっている。先ほどと比べて、データから外れ値が省かれている事が分かる。

\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{data_stats3.png}
\caption{外れ値を省いたデータの統計情報}
\end{center}
\end{figure}


\subsection{相関係数}

ここで、今回使用するデータの相関関係を可視化する。それぞれの指標に対して、相関係数はFigure6のようになっている。


\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{corr.png}
\caption{相関係数}
\end{center}
\end{figure}

相関係数は以下のような数式で表され、数値が1に近いほど相関があることを示している。

\begin{center}
\begin{math}
r_{xy} = \frac{\displaystyle \sum_{i = 1}^n (x_i - \overline{x})
(y_i - \overline{y})}{\sqrt{\displaystyle \sum_{i = 1}^n 
(x_i - \overline{x})^2}\sqrt{\displaystyle \sum_{i = 1}^n 
(y_i - \overline{y})^2}}
\end{math}
\end{center}

上記の表から、各種目（ベンチプレス・スクワット・デッドリフト）の重量はそれぞれ強い相関があることを示しており、特にスクワットとデッドリフトとの間に0.9以上の強い相関がある事が分かる。また、年齢やギアと重量には相関が少ない事が伺える。

\subsection{データの正規化}

次に、このままではデータに様々な値が存在するためにデータが扱いにくいという問題点があるため、正規化(Normalization)という手法で全てのデータを0.0から1.0の値に収まるように変換を加える必要がある。正規化は、一般的に次のように表され、Python上ではListing4のように書く事ができる。

\begin{center}
\begin{math}
x_{norm(i)} = \frac{x_i - x_min }{ x_max - x_min } 
\end{math}
\end{center}


\begin{python}[caption=データの正規化]
sc = MinMaxScaler()
data_std = sc.fit_transform(data)
\end{python}

この手法でデータを扱いやすい形に変換する事ができたが、この手法は外れ値の影響を受けやすいという欠点が存在する。しかし、先ほど外れ値はデータから取り除いているので、今回のデータに対して正規化は妥当なデータ処理手法であると言える。なお、正規化を施した後のデータセットは、以下のようになっている

\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{data_normalized.png}
\caption{正規化後のデータセット}
\end{center}
\end{figure}

\newpage
\section{評価指標}

回帰分析とは、説明変数\begin{math}X\end{math}に対し目的変数\begin{math}Y\end{math}が得られる分析の事である。したがって、正解値\begin{math}T\end{math}を目的変数\begin{math}Y\end{math}がどの程度正確に予測できいるかを調べる必要がある。今回の研究では、学習モデルの評価には決定係数(\begin{math}R^2\end{math})と二乗平均平方根誤差(RMSE)、そして平均絶対誤差(MAE)の3つの指標を用いることとする。

\subsection{決定係数}

決定係数は予測値\begin{math}Y\end{math}と正解値\begin{math}T\end{math}の相関を表す。一般的に決定係数は\begin{math}R^2\end{math}で示され、以下の数式で表される。

\begin{center}
\begin{math}R^2=1-\frac{\sum_{i=1}^{n}(T_i-\hat{Y_i})^2}{\sum_{i=1}^{n}(T_i-\bar{Y_i})^2}\end{math}
\end{center}

今回の実験ではモデルの構築にKerasというライブラリを使用するが、Kerasには決定係数\begin{math}R^2\end{math}が提供されていないので、以下のように定義する。
\\

\begin{python}[caption=決定係数]
def r_square(y_true, y_pred):
    from keras import backend as K
    SS_res =  K.sum(K.square(y_true - y_pred)) 
    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) 
    return (1 - SS_res/(SS_tot + K.epsilon()))
\end{python}

\begin{math}R^2\end{math}は予測値\begin{math}Y\end{math}と正解値\begin{math}T\end{math}が完全に一致する場合に１となり、１に近いほど精度の高い予測が行えていることを表す。


\subsection{二乗平均平方根誤差}

二乗平均平方根誤差(RMSE)とは後述する平均絶対誤差と並んで代表的な誤差関数の一つであり、以下のように表される。


\begin{center}
\begin{math}
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n}(T_i - Y_1)^2} 
\end{math}
\end{center}

二乗平均平方根誤差は0に近いほど予測精度が高い事を表す。こちらも決定係数同様Kerasでは提供されていないため、以下のように定義する。
\\


\begin{python}[caption=二乗平均平方根誤差]
def rmse(y_true, y_pred):
    from keras import backend as K
    return K.sqrt(K.mean(K.square(y_pred - y_true),
    axis=-1)) 
\end{python}






\subsection{平均絶対誤差}

平均絶対誤差（MAE）は二乗平均平方根誤差と並んで代表的な誤差関数の一つであり、以下のような数式で表される。

\begin{center}
\begin{math}
MAE = \frac{\sum_{i=1}^{n}|T_i-\hat{Y_i}|}{n} 
\end{math}
\end{center}


こちらも二乗平均平方根誤差と同様値が0に近いほど精度の高い予測が出来ている事を表す。平均絶対誤差は上記2つの指標と違いKeras上でライブラリとして提供されているので、Python上で定義する必要はない。一般に二乗平均平方根誤差は正規分布を仮定した指標である一方、平均絶対誤差はラプラス分布を仮定している。すなわち、データに多くの外れ値が含まれる場合には平均絶対誤差が用いられる。

\newpage
\section{学習手法}
\subsection{パーセプトロン}

\begin{math}w\end{math}を重みベクトル、\begin{math}b\end{math}をバイアスと呼ぶ時、入力した値が隠れ層を通さずに出力されるモデルを単純パーセプトロンと呼び、以下のように表される。

\begin{center}
\begin{math}
y = f(w^{T}x+b)
\end{math}
\end{center}

この時、予測値\begin{math}Y\end{math}と正解値\begin{math}T\end{math}の誤差が最小になるように重み\begin{math}w\end{math}とバイアス\begin{math}b\end{math}を学習の過程でパラメータを調整する事ができればニューラルネットワークの学習ができているという事になる。予測値と正解値にどれだけの誤差があるのかを示す指標を損失関数\begin{math}L\end{math}と呼び、これについては5.2節で詳しく解説する事とする。関数の最大・最小を考える場合には一般的にパラメータの偏微分（＝勾配）を求める必要があり、その際に勾配法と呼ばれる手法が用いられる。勾配法とは、反復学習でパラメータを逐次的に更新し関数の値を徐々に減らしていく手法である。1回の学習でどれだけパラメータを更新するかを決定する指標を学習率(learning rate)\begin{math}\eta\end{math}、目的関数の\begin{math}\theta\end{math}に対する勾配を\begin{math}g_t\end{math}とおく時、勾配法は次のように表される。

\begin{center}
\begin{math}
\theta_{t+1} = \theta_t - \eta g_t
\end{math}
\end{center}

単純パーセプトロンでは1本の直線で分類できるデータにしか対応できないため、線形分離不可能な問題を分離する事はできない。入力・出力以外にニューロン（隠れ層）が繋がったモデルを多層パーセプトロンと呼び、この手法を用いることで線形分離不可能なデータも扱う事ができる事ができる。隠れ層には活性化関数と呼ばれる非線形変形を用いる関数を用いる必要があり、5.3節で詳しく解説することとする。勾配法では誤差逆伝播法という出力層から入力層に向かって勾配を求めていくという手法が用いられる。しかし、こちらの方法にもいくつか問題点が存在し、そのような問題を解く手法を最適化アルゴリズムと言い、5.4節で解説する。以上が、ニューラルネットワークの一連の学習過程である。


\subsection{損失関数}

ニューラルネットワークの学習において最適なパラメータを設定するためには関数の最小・最大値を考える必要がある事は前述の通りだが、その際に用いられる関数のことを一般に損失関数\begin{math}L\end{math}と呼ぶ。損失関数\begin{math}L\end{math}の誤差を用いて重みとバイアスを修正することでニューラルネットワークの学習が行われる。損失関数は予測値がどれだけ正解値から乖離しているかを表す指標であり、代表的なものに二乗和誤差や交差エントロピー誤差が挙げられる。一般に回帰問題では二乗和誤差が用いられ、分類問題では交差エントロピー誤差が用いられる。これは、二乗和誤差では正解値と予測値の乖離がそのまま誤差として表現されるのに対し、交差エントロピーでは自然対数eを底とするモデルの予測値と正解値の乗算で表されるため、予測値が離散値になり分類パターンの確率を求めるものとして使用される。二乗和誤差は、以下のように表される。


\begin{center}
\begin{math}
L = \frac{1}{2} \sum_{k=1}^{n} (y_k - t_k)^{2}
\end{math}
\end{center}

回帰分析における損失関数には二乗和誤差の他にも誤差の絶対値を取るという手法が用いられる場合もある。前者はデータが正規分布に従う時に相応しく、後者はデータに外れ値が期待される時に使用されるのが相応しいとされている。



\subsection{活性化関数}

ニューロンの線形結合後に非線形変換を行う関数のことを活性化関数と呼ぶ。活性化関数に線形関数を用いない理由は、線形な変換を使うことは隠れ層を用いなくても実現できる事に起因する。代表的な活性化関数にはStep関数やSigmoid関数が挙げられ、活性化関数にSigmoid関数が用いられるモデルのことをロジスティック回帰と言う。Sigmoid関数は以下のように表される。



\begin{center}
\begin{math}
h(x) = \frac{1}{1+e^{-x}} 
\end{math}
\end{center}


また、Sigmoid関数の微分は次のように表されるため、

\begin{center}
\begin{math}
h'(x) = \frac{1}{1+e^{-x}} * (1 - \frac{1}{1+e^{-x}})
\end{math}
\end{center}

Sigmoid関数自身で微分を表すことができるという特徴を持つ。勾配を計算する誤差逆伝播法では損失関数の微分という処理が行われるため、微分可能なSigmoid関数が活性化関数として使われるようになったという歴史的背景がある。しかし、Sigmoid関数にも問題点が存在する。Sigmoid関数はS字カーブの関数であるが、Sigmoid関数の出力が0または1に近ずくにつれて、その微分の値は0に近づく。そのため、0と1に偏ったデータ分布ではニューラルネットワークの層を深くするにつれて勾配の値が小さくなるという問題点がある。また、例え層自体が深く無い場合でも、各層の次元数が多い場合などは勾配が消失しやすくなる。この問題を解決するために、活性化関数にSigmoid関数ではなく双曲線正接関数(hyperbolic tangent function)と呼ばれる関数が用いられ、以下のように数式で表される。


\begin{center}
\begin{math}
tanh(x) =  \frac{e^{x}-e^{-x}} {e^{x}+e^{-x}} 
\end{math}
\end{center}

Sigmoid関数が\begin{math}-\infty<x<+\infty\end{math}において\begin{math}0<h(x)<1\end{math}であるのに対し、\begin{math}-1<tanh(x)<1\end{math}となっている。また、双曲線正接関数の微分は以下のように表され、\begin{math}tanh'(0)=1\end{math}で最大値となるので、Sigmoid関数の導関数\begin{math}h'(x)\end{math}の最大値\begin{math}h'(0)=0.25\end{math}と比べて勾配が消失しにくいという特徴がある。

\begin{center}
\begin{math}
tanh'(x) =  \frac{4} {(e^x+e^{-x})^{2}} 
\end{math}
\end{center}

ニューラルネットワークは分類問題と回帰問題の両方に対応が可能だが、どちらの問題を用いるかによって出力層の活性化関数を変更する必要がある。分類問題において、出力層における活性化関数は確率を表す関数でなければならないので、出力の値が0から1の間に収まるSigmoid関数が用いられる。一方、回帰問題はある入力から数値の予測を行うものであるため、出力層には受け取った値に変換を加えずにそのまま出力する恒等関数(Identity function)が用いられる。


\subsection{最適化アルゴリズム}

ニューラルネットワークにおいて、最適なパラメータを求める問題を解くことを最適化問題と言う。最適なパラメータを求めるためには偏微分（＝勾配）を手がかりにしたが、それには莫大な計算時間とメモリを要する事は前述の通りである。先ほど紹介した勾配法では、パラメータを更新するためにN個全てのデータを考慮する必要があった。この問題を解決するために使われる手法に、確率的勾配降下法という手法がが存在する。通常の勾配法ではN個のデータの全ての和を取ってからパラメータを更新したが、こちらの手法ではデータをランダムに１つ選んでパラメータを更新する。すなわち、通常の勾配法でパラメータを1回更新するのと同じ計算量でパラメータをN回更新する事ができる。この学習を繰り返し行うことで損失関数の値が徐々に下がっていく事が知られており、このデータ全体に対する反復回数をエポックと言う。

しかし、確率的勾配降下法は関数の形状が等方的でない限りは局所的な再小値に勾配が落ち着いてしまうため、効率的な方法で勾配方向へ学習を進めないという問題点がある。この問題を解決するために、移動平均を用いて現在まで進んでいたパラメータを更新していた方向に進みやすくするというモーメンタムと呼ばれる手法が用いられ、次のように表される。なお、\begin{math}v\end{math}は物理でいう速度に対応している。


\begin{center}
\begin{math}
v_t := \alpha v_{t-1} - \eta  \frac{\partial L}{\partial W}
\end{math}
\end{center}

\begin{center}
\begin{math}
W_t := W_{t-1} + v_t
\end{math}
\end{center}


勾配法のもう一つの問題点として、、学習率\begin{math}\eta\end{math}が小さすぎると学習が進まず、大きすぎると学習がうまく進まない事が挙げられる。そこで、学習の過程で学習率\begin{math}\eta\end{math}を徐々に小さくすればよい。これを擬似的に再現した手法はAdaGradと呼ばれ、、以下のように表される。


\begin{center}
\begin{math}
h_t := h_{t-1} +  \frac{\partial L}{\partial W} \cdot \frac{\partial L}{\partial W}
\end{math}
\end{center}


\begin{center}
\begin{math}
W_t := W_{t-1} - \eta \frac{1}{\sqrt{h}} \frac{\partial L}{\partial W}
\end{math}
\end{center}

hはこれまでの勾配の値を2乗和として保持し単調増加するため、学習率が徐々に小さくなっていることが伺える。上記に紹介したモーメンタム法とAddGrad法を組み合わせた手法に、Adam法というものがある。平均移動で勾配の振動を抑制し、学習率を調整する事で効率よく勾配を進める事ができ、現在の機械学習では主流となって使用されている最適化アルゴリズムである。

\newpage
\section{実験結果}

\subsection{下準備}

実際に、先述したデータセット、評価指標、そして学習手法を利用し、ベンチプレスの回帰予測の精度を確かめてみる。今回の実験ではPyhtonのバージョン3.8.6系を使用し、実行環境はDocker Hubが提供するjupyter/datascience-notebookを使用する。まずは必要なライブラリ群をインポートする。
\\


\begin{python}[caption=必要なライブラリのインポート]
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import pandas
from tensorflow.keras.models import Sequential
import tensorflow as tf
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras import optimizers
from keras.callbacks import EarlyStopping
from keras.layers.normalization import BatchNormalization
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras import optimizers
from keras.callbacks import EarlyStopping
from keras.layers.normalization import BatchNormalization
\end{python}


データセットについては第3章で外れ値の除去などの前処理を施してあるので、そちらのデータをListing8のように訓練データとテストデータに分け、訓練データからさらに検証データを作成する。また、二乗平均平方根誤差と平均絶対誤差については第4章で定義した関数を使用する。
\\


\begin{python}[caption=訓練データ、テストデータ、検証データを作成する]
X_train, X_test, Y_train, Y_test = 
train_test_split(data_std[:,:-1], data_std[:,-1], test_size=0.2)
X_train, X_validation, Y_train, Y_validation = 
train_test_split(X_train, Y_train, test_size=0.2)
\end{python}


ここまでで下準備が整ったので、次節では実際にモデルを構築する。

\subsection{単純パーセプトロン}

まずは隠れ層を用いない単純パーセプトロンを使用する。特徴量の数は、今回使用するデータセットからベンチプレスのカラムを除いた6つとなっており、出力層には恒等関数、損失関数には二乗和誤差を使用する。この3つは今後の実験においても変わらない。エポック数は20とし、最適化アルゴリズムには確率的勾配降下法を用いる。
\\

\begin{python}[caption=単純パーセプトロン]
n_features = 6
n_hidden   = 4
model = Sequential()
model.add(InputLayer(input_shape=(n_features,)))
model.add(Dense(1))
model.add(Activation('linear'))
optimizer = optimizers.SGD(learning_rate=0.1)
model.compile(optimizer=optimizer,
              loss='mse', metrics=['mae', rmse, r_square])
log = model.fit(X_train, Y_train, epochs=20, batch_size=64, 
verbose=1,
         validation_data=(X_validation, Y_validation))
Y_pred = model.predict(X_test)
\end{python}

最終的な出力は平均絶対誤差の値が0.0510、二乗平均平方根誤差の値が0.0510、決定係数の値が0.8468となっており、この段階で既に非常に精度の高いモデルが構築できている事が分かる。最終的に平均絶対誤差と二乗平均平方根誤差の値が0.05を下回る事を目標に、ニューラルネットワークに改良を加えていく。

\subsection{Sigmoid関数}

それでは最初に、隠れ層を1つ追加し、先ほどと同様の実験を行ってみる。なお、隠れ層で用いる活性化関数はSigmoid関数で、ニューロンの数は4つとする。
\\

\begin{python}[caption=Sigmoid関数]
n_features = 6
n_hidden   = 4
model = Sequential()
model.add(InputLayer(input_shape=(n_features,)))
model.add(Dense(n_hidden, activation='sigmoid'))
model.add(Dense(1))
model.add(Activation('linear'))
optimizer = optimizers.SGD(learning_rate=0.1)
model.compile(optimizer=optimizer,
              loss='mse', metrics=['mae', rmse, r_square])
log = model.fit(X_train, Y_train, epochs=20, batch_size=64, 
verbose=1,
         validation_data=(X_validation, Y_validation))
Y_pred = model.predict(X_test)
Y_pred = model.predict(X_test)
\end{python}

この時、最終的な出力は平均絶対誤差の値が0.0510、二乗平均平方根誤差の値が0.0510、決定係数の値が0.8491 となっており、先ほどから僅かに精度が向上している事が伺える。ここで、訓練データとテストデータに対して精度がどのように推移しているか可視化してみる。
\\

\begin{python}[caption=Sigmoid関数を用いた学習推移]
plt.plot(log.history['val_r_square'])
plt.plot(log.history['r_square'])
plt.title('model R^2')
plt.ylabel('R^2')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(log.history['mae'])
plt.plot(log.history['val_mae'])
plt.title('mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
\end{python}


\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{experiment1.png}
\caption{実験結果1}
\end{center}
\end{figure}

上記のグラフから、決定係数の学習が初期の段階で頭打ちになっている事が伺える。勾配が途中で消失している可能性があるので、次節では活性化関数にSigmoid関数の代わりに双曲線正接関数を使用し、同様の実験を行ってみる。

\subsection{双曲線正接関数}

双曲線正接関数を用いた学習は、Python上で次のように表される。
\\

\begin{python}[caption=双曲線正接関数]
n_features = 6
n_hidden   = 4
model = Sequential()
model.add(InputLayer(input_shape=(n_features,)))
model.add(Dense(n_hidden, activation='tanh'))
model.add(Dense(1))
model.add(Activation('linear'))
optimizer = optimizers.SGD(learning_rate=0.1)
model.compile(optimizer=optimizer,
              loss='mse', metrics=['mae', rmse, r_square])
log = model.fit(X_train, Y_train, epochs=20, batch_size=64, 
verbose=1,
         validation_data=(X_validation, Y_validation))
\end{python}

この時、最終的な出力は平均絶対誤差の値が0.0508、二乗平均平方根誤差の値が0.0508、決定係数の値が0.8507となっている。決定係数の値は0.0079ほど向上しているが、逆に平均絶対誤差と二乗平均平方根誤差の値は先ほどより0.0003ほど悪化している事が伺える。次節では隠れ層の数を増やして同様の実験を行う事とする。


\subsection{隠れ層あり双曲線正接関数}


双曲線正接関数を活性関数として用いた隠れ層を増やすことに伴い、エポック数のの数を30に増やすこととする。
\\


\begin{python}[caption=隠れそうあり双曲線正接関数]
n_features = 6
n_hidden   = 4
model = Sequential()
model.add(InputLayer(input_shape=(n_features,)))
model.add(Dense(n_hidden, activation='tanh'))
model.add(Dense(n_hidden, activation='tanh'))
model.add(Dense(n_hidden, activation='tanh'))
model.add(Dense(1))
model.add(Activation('linear'))
beta_2=0.999, amsgrad=True)
optimizer = optimizers.SGD(learning_rate=0.1)
model.compile(optimizer=optimizer,
              loss='mse', metrics=['mae', rmse, r_square])
log = model.fit(X_train, Y_train, epochs=30, batch_size=64, 
verbose=1,
         validation_data=(X_validation, Y_validation))
Y_pred = model.predict(X_test)
Y_pred = model.predict(X_test)
\end{python}


この時、最終的な出力は平均絶対誤差の値が0.0503、二乗平均平方根誤差の値が0.0503、決定係数の値が0.8507となっている。決定係数は先ほどから変化していないが、平均絶対誤差と二乗平均平方根誤差が僅かに向上している事が伺える。しかし、現時点では目標の0.06には届いていない。それでは最後に、隠れ層を1つ増やし、最適化アルゴリズムに確率的勾配降下法ではなくAdamを使用し実験を試みてみる。

\subsection{Adam}




\begin{python}[caption=最適化アルゴリズムの適用]
n_features = 6
n_hidden   = 4
model = Sequential()
model.add(InputLayer(input_shape=(n_features,)))
model.add(Dense(n_hidden, activation='tanh'))
model.add(Dense(n_hidden, activation='tanh'))
model.add(Dense(n_hidden, activation='tanh'))
model.add(Dense(n_hidden, activation='tanh'))
model.add(Dense(1))
model.add(Activation('linear'))
optimizer = optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer,
              loss='mse', metrics=['mae', r_square])
log = model.fit(X_train, Y_train, epochs=30, batch_size=64, 
verbose=1,
         validation_data=(X_validation, Y_validation))
\end{python}

この時、最終的な出力は平均絶対誤差の値が0.0485、二乗平均平方根誤差の値が0.0485、決定係数の値が0.8626 となっており、無事平均絶対誤差と二乗平均平方根誤差共に目標値に達し、決定係数も大幅に向上している事が確認できた。それでは最後に学習のグラフを可視化してみる。
\\


\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{experiment2.png}
\caption{実験結果2}
\end{center}
\end{figure}


上記の実験結果より、隠れ層や最適化アルゴリズムを改良することにより機械学習モデルの精度を向上させる事ができる事が確認できた。

\newpage
\section{むすび}

本研究ではパワーリフティングスコア、すなわち複数の説明変数からなる重回帰分析を機械学習の各手法を用いて評価したものであった。その過程で、データの正規化、外れ値の除去、二乗和誤差、双曲線正接関数、Adamなど現実の機械学習でも広く使用されている手法を紹介した。無論、今回紹介した手法はどれも最も一般的に使用されているものであり、世の中には今回紹介した手法以外にも様々な機械学習アルゴリズムが存在する。また、隠れ層の数やニューロンの数の決定指標について、定石というものは存在すれどもどの値が最適解をもたらすかについては半ば決め打ちとなっているのが現在の現状である。
\\

本研究の課題点として、回帰精度が初期の段階から高く、その後様々な手法を用いても結果が頭打ちになってしまい、各手法の精度を正確に評価する事ができなかった事が挙げられる。一般的には、単純パーセプトロンを用いたニューラルネットワークモデルでは非線形な回帰問題を解決できないことは先述の通りだが、単純パーセプトロンを用いたモデルと隠れ層を重ねた"精度が良い"と考えられる最終的なモデルでは決定係数が0.0158程度の向上に収まっている。この数値を小さいと見るか大きいとみるかは各判断に委ねるが、今回のような実験結果になってしまった原因として、使用したデータセットに元々の相関関係が強く非線形な変換を行う必要がなかった・筆者のデータセットに対する前提知識が関与してしまっている、という２点が考えられる。よって、今後の発展としては、今回用いた手法をより相関関係と筆者の前提知識が関与しないデータセットで用いる事が必要であると言える。
\\


本論文の執筆にあたり多くの方々にご支援頂きました。指導教官である稲葉知士教授には本論文の執筆にあたりゼミの時間外でも多大な時間を使って指導してくださり、感謝致します。また、東海大学航空宇宙学科2年生の須藤直太郎君には本論文に執筆に当たって助言を頂き感謝致します。最後に、稲葉ゼミの皆様にも研究の実施に当たって多くの協力を頂きました、この場を借りて感謝の意を示します。

\newpage

\addcontentsline{toc}{chapter}{Introduction}
\bibliographystyle{plain}
\begin{thebibliography}{99}
\bibitem{sincho} 太宰治、『走れメロス』、新潮（1940年5月号）
\bibitem{chikuma} 太宰治、太宰治全集3（ちくま文庫）、筑摩書房(1988).
\bibitem{schiller} Friedrich von Schiller, バラード¥textit{de:Die B¥"{u}rgschaft}, 1815.
\end{thebibliography}




%  \citep{adams1995hitchhiker}

% \bibliographystyle{plain}
% \bibliography{references}
\end{document}
